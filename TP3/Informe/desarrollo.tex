\section{Desarrollo}
\subsection{Representación de la matriz esparsa}
Dado que, como dijimos, la matriz sobre la que trabajaremos es altamente esparsa, utilizamos una estructura de datos que aproveche este aspecto para ahorrar tanto espacio en memoria como tiempo de cómputo. Para esto, representamos la matriz como un arreglo de columnas, cada una de éstas siendo una lista enlazada de pares $<i, a_{i,j}>, a_{i,j} \neq 0$. Desde el punto de vista espacial, esto nos permite almacenar únicamente las entradas no nulas de la matriz. Esta representación nos permite realizar varias de las operaciones que se requieren para construir la matriz particular sobre la que trabaja PageRank de manera eficiente al recorrer por columnas, y también nos permite ejecutar eficientemente la operación de multiplicar por un vector a derecha, que es el paso central del método de la potencia que utilizamos para calcular el autovector buscado, de la siguiente manera. Siendo:
$$Av = b \Leftrightarrow b_i = (Av)_i = \displaystyle\sum_{k=1}^n a_{i,k}v_k$$
se puede ver que $b_i$ resulta ser la acumulación de la suma de todos los $a_{i,k} v_k$.
Dado que al multiplicar un vector por una matriz se utilizan todas las filas una única vez (es decir $i$ adopta todos los valores entre $1$ y $n$), y por ende, todos los elementos de la matriz (una única vez), podemos computar lo anterior como:
$$b = 0; \text{ Para } a_{i,j},  1\leq i\leq n, 1\leq j \leq n: b_i:= b_i + a_{i,j} v_j$$
Dado que la suma es conmutativa, el orden en el que iteremos los $a_{i,j}$ no altera el resultado, con lo cual, podemos iterar ``hacia abajo'' por las columnas (aprovechando la estructura de lista, donde cada avance tiene costo constante), y esto repetirlo en todas. De esta manera, cubrimos todos los elementos de la matriz una única vez y acumulamos los resultados parciales en la coordenada correspondiente del vector resultado. Además, dado que los elementos nulos de la matriz no están representados en las listas, evitamos computar productos nulos y su acumulación. Luego, el costo de multiplicar esta matriz por un vector cualquiera depende únicamente (y de manera lineal) de la cantidad de elementos no nulos de ésta.

\subsection{Método de la Potencia}
Como ya dijimos, el método de la potencia es un método iterativo muy simple que converge hacia el autovalor de mayor módulo de la matriz (en caso de que exista uno único). Nuestra implementación es bastante directa, utilizando las estructuras que describimos antes. Consideramos como criterio de parada que la diferencia (en módulo) de la norma $1$ de dos iteraciones sucesivas sea menor que $\eps = 10^{-8}$. Consideramos que con este valor es suficiente, ya que utilizaremos el resultado únicamente para ordenar las páginas web entre sí, con lo cual, suponemos que agregar precisión en los decimales menos significativos no debería producir alteración en el orden de las páginas, que estaría dominado por las primeras cifras. Además, para evitar problemas numéricos de representación, normalizamos el vector luego de cada iteración, para controlar el rango de valores de las coordenadas.

\subsection{Extrapolación Cuadrática y Cuadrados Mínimos}
La implementación del algoritmo de Extrapolación Cuadrática es directa del presentado en la publicación citada. Lo único que debimos agregar fue la manera de resolver el sistema de cuadrados mínimos que genera, utilizando matrices ortogonales. Dado que la aproximación por cuadrados mínimos consiste en encontrar un $x$ tal que minimice $||Ax-b||_2$ y que multiplicar por matrices ortogonales no altera la norma, resulta equivalente buscar un $x$ que minimice $||QAx -Qb||_2$. Dado que la matriz $A$ que construimos tiene dimensión $n\times 2$ (con $n > 2$) y que podemos conseguir $Q$ tal que $QA$ resulte triangular superior, minimizar $||QAx -Qb||_2$ es equivalente a resolver el sistema de $2\times 2$ que resulta de tomar las dos primeras filas de $QA$ y de $Qb$, lo cual se realiza muy fácilmente. Para conseguir esta tal $Q$ adaptamos el método de Householder para construir la factorización QR de una matriz, de la siguiente manera.\\ 

El método de Householder genera matrices ortogonales $Q_j$ que, multiplicadas a la izquierda de $A$, hacen que el producto sea triangular superior. Dado que hay que construir una por cada columna de $A$, en nuestro caso construimos un sistema equivalente: $Q_2 Q_1 A x = Q_2 Q_1 b$. Dado que $Q_2 Q_1$ es una matriz ortogonal, el sistema resulta equivalente, tanto en términos de cuadrados mínimos (como dijimos antes), como en términos de la solución lineal ($Ax = b \Leftrightarrow MAx = Mb$ si $M$ es inversible). \\

Sin embargo, dado que las dimensiones de las matrices con las que debe lidiar el algoritmo de PageRank suelen ser muy grandes, construir estas matrices explícitamente no es una opción. En vez de esto, aplicamos las transformaciones que propone el método directamente sobre las columnas de la matriz y sobre el término independiente (dado que sabemos que son equivalentes a la multiplicación por matrices ortogonales). En particular: sabemos que $Q_1 = Id - 2 u_1u_1^t$, donde $u_1 = a_1 - ||a_1||_2 e_1$ normalizado, siendo $a_1$ la primera columna de $A$ y $e_1$ el primer vector de la base canónica de $\R^n$, y que el efecto de hacer $Q_1 A$ es anular toda la primera columna, excepto su primer elemento, que se convierte en la norma de la columna antes de la modificación, y convertir $a_2$ en $Q_1 a_2$. Además, dado que la misma transformación la debemos aplicar sobre el término independiente, convertimos $b$ en $Q_1 b$. La operación sobre $a_1$ no es necesario computarla, simplemente corregimos los valores de la matriz según lo que dijimos antes. Los otros productos sí debemos calcularlos, aunque podemos observar lo siguiente: dado que $Q_1 = Id -2u_1u_1^t$, al multiplicar a derecha por un vector $v$ obtenemos $Q_1 v = v - 2 u_1 u_1^t v =v - 2(u_1^t v)u_1$, pues $u_1^t v\in \R$. Luego, tanto para el caso de $a_2$ como para $b$, computar esto no resulta costoso: basta con calcular el producto interno con $u_1$ (lo que tiene costo lineal en la dimensión de los vectores) y luego restar del vector original que estamos considerando: $v_i := v_i - 2 <u_1, v> (u_1)_i$. Teniendo esto calculado, procedemos de manera análoga para la segunda columna, recordando que no debemos considerar la primera fila tanto de $Q_1 A$ como de $Q_1 b$. Finalmente, tenemos un sistema triangular superior, equivalente al original, que podemos resolver mediante backward-substitution en dos pasos. 
 
\subsection{Construcción de la matriz de PageRank}
Para construir la matriz sobre la cual trabajaremos, primero leemos una a una las líneas de la entrada, colocando un $1$ en la posición indicada por cada una. Luego, dado que la matriz debe ser estocástica por columnas, recorriendo éstas una a una dividimos cada valor por la suma de todos sus elementos (ésta es otra de las ventajas de tener la matriz representada por columnas). Como dijimos en la introducción, no realizamos más modificaciones sobre la matriz; los valores de corrección para considerar \emph{dangling-nodes} no los representamos directamente, sino que son consecuencia del manejo de algebraico particular de los vectores, según lo explica el algoritmo que estamos considerando.