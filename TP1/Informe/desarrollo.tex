\section{Desarrollo}
	\subsection{Métodos propuestos}
%decir que son métodos mixtos. Que nos gusta la bisección porque nos asegura convergencia en caso de que lo necesitemos y porque no mete errores de propagación	
	Según lo consignado en el enunciado, exponemos a continuación los distintos métodos propuestos para computar $\frac{1}{\sqrt{\alpha}}$
	%Métodos generales -> Demostrar Newton, convergencia.
		\subsubsection{Cero de $f(x) = x^2 -\alpha$}
		Una alternativa propuesta en el enunciado consiste en calcular primero $\sqrt{\alpha}$ como un cero de la función $f(x) = x^2 -\alpha$, y luego calcular su inverso multiplicativo. Para esto utilizaremos el método de Newton (\ref{prop_newton}). Con lo cual, para encontrar un cero de $f$ debemos buscar un punto fijo de 
		\begin{equation}
			g(x) = x - \frac{f(x)}{f'(x)} = x - \frac{x^2 - \alpha}{2x} = \frac{x + \frac{\alpha}{x}}{2}
		\label{g}	
		\end{equation}
lo cual realizaremos construyendo la sucesión
	\begin{eqnarray}
		&& x_0 \in [a,b], \sqrt{\alpha} \in [a,b] \nonumber \\
		&& x_{n+1} = g(x_n) 
		\label{sqrtNewt}
	\end{eqnarray}
cuya convergencia dependerá de los valores de $a,b$. 

Demostramos el siguiente resultado teórico:

\begin{prop} [Convergencia del Método (Ej 3.a)]
Sea $\{x_n\}_{n\in\N}$ definida como en \ref{sqrtNewt}. Si $x_0 > \sqrt{\alpha}$, la sucesión converge cuadráticamente a $\sqrt{\alpha}$.
\begin{lema}
$x_n > \sqrt{\alpha}$ para cualquier $ n \geq 0$
\begin{proof}
Observemos primero la siguiente equivalencia:
$$x_{n+1} = \displaystyle \frac{1}{2} (x_{n} + \frac{\alpha}{x_{n}}) < x_n \Leftrightarrow \displaystyle \frac{1}{2} (1 + \frac{\alpha}{x^2_{n}}) < 1 \Leftrightarrow \frac{\alpha}{x_n^2} < 1 \Leftrightarrow \sqrt{\alpha} < x_n$$

Veamos que efectivamente se verifica que todos los términos de la sucesión están acotados inferiormente por $\sqrt{\alpha}$. Sabemos que los términos se construyen como $x_{n+1} =  \frac{x_n + \frac{\alpha}{x_n}}{2} = g(x_n)$, con lo cual veamos que podemos acotar inferiormente a $g$.

$$g(x) > \sqrt{\alpha} \Leftrightarrow \frac{1}{2}(x + \frac{\alpha}{x}) > \sqrt{\alpha} \Leftrightarrow  \alpha > -x^2 + 2x\sqrt{\alpha} \Leftrightarrow  0 > -x^2 + 2x\sqrt{\alpha} - \alpha \Leftrightarrow  0 < x^2 - 2x\sqrt{\alpha} + \alpha = (x-\sqrt{\alpha})^2$$  

Observando el signo de la desigualdad, al tratarse de un cuadrado, si el binomio no se anula, podemos afirmar la cota. Es decir:
$$g(x) > \sqrt{\alpha} \Leftrightarrow x \neq \sqrt{\alpha}$$

 Veamos que efectivamente esto no sucede para la sucesión, por inducción en $n$. Para $n = 0$, por hipótesis sabemos $x_0 > \sqrt{\alpha}$, con lo cual la diferencia no es nula. Para $x_n$, con $n\geq 1$ tenemos $x_n = g(x_{n-1})$. Como $x_{n-1}\neq \sqrt{\alpha}$ por hipótesis inductiva, se sigue$  \sqrt{\alpha} < g(x_{n-1}) = x_n$, como necesitábamos ver.
\end{proof}
\end{lema}
\begin{proof}
Veamos, ahora sí, que efectivamente el método converge. Para eso, veremos que se satisfacen las hipótesis del lema \ref{lema_f}.
\begin{description}
	\item[Derivabilidad] La función obtenida es continua y derivable en los reales estrictamente positivos, con lo cual esto se satisface en todos los intervalos sobre los que se trabajará.
	\item[Derivada acotada]  Necesitamos $g'(x) \leq k < 1$, con lo cual veamos $g'(x) = \frac{1}{2} (1- \frac{\alpha}{x^2})$. Sabemos ya que $x > \sqrt{\alpha}$, con lo cual, $ 0<\frac{\alpha}{x^2} < 1$, o sea $0 < g'(x) < \frac{1}{2}$.
	\item[Preservación del intervalo] Necesitamos $g([a,b]) \inc [a,b]$. Sabemos que $g$ puede tener un extremo en $\sqrt{\alpha}$, pues se anula su derivada primera. Miremos $g''(x) = \frac{\alpha}{x^3}$ para comprobarlo. Esta función es estrictamente positiva en $\R_{>0}$, con lo cual efectivamente se trata de un extremo y un mínimo. Además, podemos ver que si $x > \sqrt{\alpha}$ la derivada es positiva, y por lo tanto, la función creciente. Con lo cual, sea $\eps$  tal que $g(\sqrt{\alpha}) \leq g(x) \forall x\in [\sqrt{\alpha}-\eps, \sqrt{\alpha}-\eps]$, condición que sabemos que sucede por tratarse de un extremo local, sea $b > \sqrt{\alpha}$. Luego, $g([a,b]) = [g(\sqrt{\alpha}), g(b)]$, ya que, como la función es estrictamente creciente a la derecha del mínimo, el valor máximo lo alcanzará en el valor máximo de $x$, y por ser continua, tomará todos los valores intermedios entre su máximo y su mínimo. Ya sabemos $a = \sqrt{\alpha} -\eps < g(\sqrt{\alpha}) = \sqrt{\alpha}$, falta ver $g(b) < b$, pero imaginando $b = x_0$, esto vale por el lema anterior.
	
\end{description}
\end{proof}

\end{prop}
Además de esto, utilizamos otro argumento teórico para establecer un intervalo donde el método converja con seguridad. 
 Miremos ahora $ g'(x) = \frac{1}{2} - \frac{\alpha/2}{x^2}$.\\
$$ |g'(x)| = |\frac{1}{2} - \frac{\alpha/2}{x^2}|  < 1  
\Leftrightarrow  -1 < \frac{1}{2} - \frac{\alpha/2}{x^2} < 1 
	\Leftrightarrow  -\frac{3}{2} <  - \frac{\alpha/2}{x^2}  < \frac{1}{2}  \Leftrightarrow 3 >  \frac{\alpha}{x^2}  
	\Leftrightarrow  \frac{\sqrt{\alpha}}{\sqrt{3}} < x $$\\
Antes que nada, observemos que descartamos una de las desigualdades pues se cumple siempre por los signos de las expresiones. También sabemos $1/\sqrt{3} < 0.58 + \eps$, con lo cual tomando $x > 0.58\sqrt{\alpha} > \sqrt{\alpha}/\sqrt{3} + \eps'$ se verfica lo que necesitamos. Además, nuestro intervalo debe contener a $\sqrt{\alpha}$, con lo cual siempre vale $a \leq \sqrt{\alpha} \leq b$, por lo que si tomamos $a = 0.58b$ tenemos lo siguiente:
$$x \geq a \Leftrightarrow x \geq 0.58b \geq 0.58\sqrt{\alpha} \Rightarrow x > \sqrt{\alpha}/\sqrt{3} + \eps$$ 
como necesitábamos. 

Veamos ahora $g([a,b]) \inc [a,b]$. Observemos que $g$ tiene un único extremo en $\sqrt{a}$ y éste es un mínimo, pues $g'(\sqrt{\alpha}) = \frac{1}{2} - \frac{1}{2}\frac{\alpha}{(\sqrt{\alpha})^2} = \frac{1}{2} - \frac{1}{2}\frac{\alpha}{\alpha} = 0$ y además $g''(x) = \frac{\alpha}{x^3} >0$ para cualquier $x>0$. Luego, como se trata de unafunción continua, $g([a,b]) = [\sqrt{\alpha}, \max(g(a), g(b))]$. Siempre suponemos que $\sqrt{\alpha}\in[a,b]$, con lo cual basta asegurar $\max(g(a), g(b))\leq b$ para conseguir lo deseado.

Utilizaremos este último argumento ya que nos provee una forma explícita para los extremos, además de que se presta para ir obteniéndolos por sucesivas bisecciones desde un intervalo original, fácil de determinar. Como dijimos, de todas formas, sabemos que este proceso puede ser costoso en cuanto a tiempos, por lo que, experimentalmente, terminaremos de establecer cotas para los extremos, o limitar su cantidad de iteraciones.

		\subsubsection{Cero de $e(x) = \frac{1}{x^2} - \alpha$ - M\'etodo de Newton}
		La segunda alternativa es considerar a la función $e(x) = \frac{1}{x^2} - \alpha$ y buscar una aproximación a una de sus raíces: $\frac{1}{\sqrt{\alpha}}$, el valor que precisamente queríamos calcular en primera instancia. Para aproximar dicha raíz, planteamos dos métodos. El primero es aplicar el método de Newton para la función $e(x)$. Entonces, como vinimos explicando, buscamos hallar un punto fijo para la función
		\begin{equation}
			g(x) = x - \frac{e(x)}{e'(x)} = x - \frac{\frac{1}{x^2} - \alpha}{\frac{-2}{x^3}} = x + \frac{x - \alpha x^3}{2} = \frac{3x - \alpha x^3}{2}
		\label{g_e}
		\end{equation}
analizando la convergencia de la sucesión
		\begin{eqnarray}
		  && x_0 \in [a,b], \frac{1}{\sqrt{\alpha}} \in [a,b] \nonumber \\
		  && x_{n+1} = g(x_n) \nonumber
		\end{eqnarray}
		
Nuevamente, para asegurar la convergencia, buscamos un intervalo $[a,b]$ que satisfaga las hipótesis del Lema \ref{lema_f}.
  En primer lugar, $g(x)$ está bien definida, es continua y derivable en el intervalo $(0, +\infty)$ por ser un polinomio. Como mencionamos anteriormente, $\frac{1}{\sqrt{\alpha}}$ no está definida para $\alpha \leq 0$, con lo cual asegurar continuidad y derivabilidad en $(0,+\infty)$ es suficiente.\\
  
  Analicemos $|g'(x)| = |\frac{3}{2} - \frac{3}{2}\alpha x^2|$:\\\\
  $$
	 |g'(x)| < 1 
	 \Leftrightarrow |\frac{3}{2} - \frac{3}{2}\alpha x^2| < 1 
	 \Leftrightarrow -1 < \frac{3}{2} - \frac{3}{2}\alpha x^2 < 1
	 \Leftrightarrow \frac{1}{3\alpha} < x^2 < \frac{5}{3\alpha}
	 \Leftrightarrow \frac{1}{\sqrt{3}\sqrt{\alpha}} < x < \frac{\sqrt{5}}{\sqrt{3}\sqrt{\alpha}} \mbox{ siempre } x > 0
$$
  Ahora consideremos $0 < \alpha < 1$, esto implica que $\alpha < \sqrt{\alpha}$ y $\sqrt{\alpha} < \frac{1}{\alpha} \Leftrightarrow \alpha < \frac{1}{\sqrt{\alpha}}$.  Luego, 
$$      \frac{1}{\sqrt{3}\sqrt{\alpha}} < x < \frac{\sqrt{5}}{\sqrt{3}\sqrt{\alpha}} \Leftrightarrow \frac{\alpha}{\sqrt{3}} < x < \sqrt{\frac{5}{3}}\frac{1}{\alpha} \\
$$ 
  Por otro lado, si $\alpha \geq 1$, $\alpha \geq \sqrt{\alpha} \Leftrightarrow \alpha \geq \frac{1}{\sqrt{\alpha}}$. Para este caso podemos afirmar entonces:
	  $$	      \frac{1}{\sqrt{3}\sqrt{\alpha}} < x < \frac{\sqrt{5}}{\sqrt{3}\sqrt{\alpha}} \Leftrightarrow \frac{1}{\sqrt{3}\alpha} < x < \sqrt{\frac{5}{3}}\alpha $$
	  
Realizaremos un análisis similar al anterior para asegurar $g([a,b]) \inc [a,b]$. Sea $p = \frac{1}{\sqrt{\alpha}}$, miremos $g'(x) = \frac{3-3\alpha x^2}{2}$. Ya sabemos $g'(p) = 0$, miremos $g''(p) =  -3 \alpha x$, que es estrictamente negativa en los números positivos, y por lo tanto, el punto crítico es un máximo. Con un argumento similar al que utilizamos en métodos anteriores, sabemos $g([a,b]) \inc [min(g(a), g(b)), p]$. Podemos ver que $g(x) > x$ para cualquier $0<x<\frac{1}{\alpha}$, con lo cual ya tenemos $g(a) > a$. También se puede deducir fácilmente que $g(b) \leq b$, tanto para cualquiera de los dos casos del valor de $\alpha$.


\subsubsection{Cero de $e(x) = \frac{1}{x^2} - \alpha$ - Punto fijo}
Además de buscar esta raíz con el método de Newton, propusimos otra función cuyo punto fijo sea el cero buscado. Comenzamos tomando $g(x) = \frac{1}{\alpha} \frac{1}{x}$, que tiene un punto fijo en  $p = \frac{1}{\sqrt{\alpha}}$ y $e(p)= 0$. Sin embargo, $g'(x) = -\frac{1}{\alpha} \frac{1}{x^2}$ no se anula en $p$, con lo cual su orden de convergencia es lineal. Dado que ya contábamos con un método cuadrático, decidimos transformar $g$ para obtener convergencia cuadrática anulando su derivada primera. Planetamos lo siguiente: sea $h = g + t \cdot s$, siendo $t(p) = 0$. Por un lado, esto garantiza que $p$ sea punto fijo de $h$, ya que $h(p) = g(p) + t(p)s(p) = g(p) + 0\cdot s(p) = g(p) = p$, y por otro nos resultaría fácil de encontrar, ya que habíamos considerado múltiples funciones que se anulan en el punto buscado. Mejor aún es cómo hace que se comporte $h$ al derivarla: $h' = g' + t'\cdot s + t \cdot s'$, que evaluado en $p$ resulta: $h'(p) = g'(p) + t'(p)s(p) + 0\cdot s'(p) = g'(p) + t'(p)s(p)$, lo cual elimina cualquier restricción posible sobre la derivada de $s$. Propusimos $t(x) = e(x)$, con lo cual, resulta $h'(p) = g'(p) + e'(p)s(p) = 1 + e'(p)s(p)$. Si queremos $h'(p) = 0$ basta pedir $e'(p)s(p) = -1$, que se satisface fácilmente tomando $s = \frac{-1}{e'}$. Siendo $e'(x) = \frac{-2}{x^3}$, tenemos $s(x) = \frac{x^3}{2}$.

Finalmente, tenemos 
\begin{equation}
	h(x) = \frac{1}{\alpha} \frac{1}{x} + (\frac{1}{x^2} - \alpha)\frac{x^3}{2} =  \frac{1}{\alpha x} + \frac{x - \alpha x^3}{2}
\end{equation}
que, por cómo la construimos, convergerá cuadráticamente cuando le apliquemos el método de punto fijo.

En este caso no demostraremos el intervalo de convergencia, simplemente procedemos a ajustar el intervalo bisectando la derivada, hasta que en ambos extremos se verifique $|g'(x)| < 1$.

\subsubsection{Bisección de $x^2-\alpha$}
Aprovechando que ya teníamos implementado el procedimiento de bisección, lo aplicamos a buscar el cero de $f(x) = x^2-\alpha$. La implementaión es tri



\subsection{//UN TÍTULO PARA ESTO}
	
Como expusimos anteriormente (OBVIO), en todos los casos en que aplicamos el método de Newton tenemos argumentos teóricos para asegurar su convergencia en orden cuadrático, a costo de reducir iterativamente por bisección un intervalo dentro del cual que podamos asegurar que la raíz buscada existe hasta que satisfaga las hipótesis del lema. Aprovecharemos esta garantía para fijar el criterio de parada de los métodos, una vez definido el cual y conociendo el desempeño del algoritmo en un caso teóricamente óptimo nos dedicaremos a relajar los parámetros que aseguran convergencia para determinar, experimentalmente, cotas para éste que representen mejor un compromiso entre calidad de la solución y tiempo de ejecución.

	\subsubsection{Función de control}
Para medir la calidad de la solución, implementamos la función $f(x) = \frac{1}{\sqrt{x}}$ directamente sobre el hardware, mediante una pequeña rutina de lenguaje ensamblador. Ésta utiliza únicamente las funciones cociente y raíz cuadrada del procesador, con lo cual confiamos en que, si bien también es una aproximación, es una de las mejores aproximaciones que podemos esperar dada la máquina.

	\subsubsection{Criterio de parada}
Establecimos como principal criterio de parada un umbral para el error relativo entre iteraciones sucesivas. Además, agregamos una condición de corte por cantidad de iteraciones para evitar problemas en eventuales casos patológicos que no hayamos contemplado en este análisis. Para determinar el umbral, realizaremos nuestras aproximaciones para las raíces pedidas tomando varios valores de $\alpha$ dentro de distintos órdenes de magnitud, repitiendo esto para distintos órdenes de magnitud de tolerancia. Luego, promediaremos los errores relativos de las soluciones dentro de cada orden de magnitud, a partir de lo cual compararemos los resultados obtenidos para las distintas tolerancias <redactame>, buscando, por un lado, descartar sobreexigencias, es decir, tolerancias <redactame> muy pequeñas que no aporten significativamente a la calidad de la solución, a la vez que otras que, aunque tal vez lo hagan, impliquen excesivo tiempo de cómputo. Este último caso será el más difícil de determinar, y de darse nos 
obligará a realizar nuevos experimentos para establecer el compromiso entre calidad y eficiencia.

	\subsubsection{Performance}
Como comentamos en la sección anterior (OBVIO), nuestros métodos cuentan con dos etapas: una en la cual ajustan un intervalo alrededor de la raíz mediante sucesivas bisecciones y otra donde efectivamente aplican un método iterativo de orden de convergencia mayor. Sabemos que la condición expresada en el lema \ref{lema_f} es suficiente pero no necesaria, con lo cual probablemente sea excesivamente exigente en la práctica. De todas maneras, un intervalo pequeño hace que los métodos cuadráticos converjan en muy pocas iteraciones, con lo cual no nos parece una mala idea invertir iteraciones tratando de ajustar las cotas iniciales si esto se verá reflejado en la cantidad de iteraciones que necesitará el método cuadrático para converger.

Para determinar este balance comenzaremos observando cuántas iteraciones realiza cada parte del algoritmo para valores que produzcan intervalos iniciales particularmente grandes, y cómo varía esta proporción a medida que se restringen las primeras. Una vez conocido esto (en principio, a grandes razgos), mediremos el tiempo que efectivamente tardan en computarse, ya que no podemos suponer que los dos tipos de iteraciones sean igual de costosos computacionalmente. Teniendo esta variable en mente, realizaremos el mismo tipo de ajustes que planteamos antes <redactame>.

	\subsubsection{Comparación final}
Una vez ajustados los mejores parámetros de cada método en base a los valores obtenidos experimentalmente, compararemos los métodos entre sí. Además de los aspectos que ya comentamos para cada método por separado, compararemos las aproximaciones obtenidas por cada uno sobre una batería de casos (EEEEPA), donde pretendemos diferenciar su desempeño (tanto en calidad de la solución como en velocidad de cómputo) para diferentes valores de entrada, eligiendo éstos especialmente para poner a prueba diferentes debilidades posibles (intervalo inicial muy grande, inestabilidad numérica (EEEEPA), etc), así como también sobre valores aleatorios dentro de un rango suficientemente grande para considerarlo representativo de uso. Con estos datos intentaremos establecer cuál es el método más conveniente para utilizar, y si esto depende y cómo de los valores de entrada.


